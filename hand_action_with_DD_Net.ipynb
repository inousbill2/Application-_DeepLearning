{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hand action with DD-Net.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/inousbill2/Application-_DeepLearning/blob/master/hand_action_with_DD_Net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBLSQB8bRtdT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import scipy.ndimage.interpolation as inter\n",
        "from scipy.signal import medfilt \n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "from keras.optimizers import *\n",
        "from keras.models import Model\n",
        "from keras.layers import *\n",
        "from keras.layers.core import *\n",
        "from tensorflow.keras.callbacks import *\n",
        "from keras.layers.convolutional import *\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "\n",
        "import google.colab.files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4Dd8w2Uaks3",
        "colab_type": "text"
      },
      "source": [
        "1. Define configurations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyxDYnOJaotB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random.seed(1234)\n",
        "\n",
        "class Config():\n",
        "    def __init__(self):\n",
        "        self.frame_l = 32 # the length of frames\n",
        "        self.joint_n = 15 # the number of joints\n",
        "        self.joint_d = 2 # the dimension of joints\n",
        "        self.clc_num = 21 # the number of class\n",
        "        self.feat_d = 105\n",
        "        self.filters = 64\n",
        "C = Config()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KG-n2OiaF4h",
        "colab_type": "text"
      },
      "source": [
        "2. Define data processing functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gToZ5f6haNKs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Temple resizing function\n",
        "def zoom(p,target_l=64,joints_num=25,joints_dim=3):\n",
        "    l = p.shape[0]\n",
        "    p_new = np.empty([target_l,joints_num,joints_dim]) \n",
        "    for m in range(joints_num):\n",
        "        for n in range(joints_dim):\n",
        "            p_new[:,m,n] = medfilt(p_new[:,m,n],3)\n",
        "            p_new[:,m,n] = inter.zoom(p[:,m,n],target_l/l)[:target_l]         \n",
        "    return p_new\n",
        "\n",
        "\n",
        "\n",
        "# Calculate JCD feature\n",
        "def norm_scale(x):\n",
        "    return (x-np.mean(x))/np.mean(x)\n",
        "  \n",
        "def get_CG(p,C):\n",
        "    M = []\n",
        "    iu = np.triu_indices(C.joint_n,1,C.joint_n)\n",
        "    for f in range(C.frame_l): \n",
        "        d_m = cdist(p[f],p[f],'euclidean')       \n",
        "        d_m = d_m[iu] \n",
        "        M.append(d_m)\n",
        "    M = np.stack(M) \n",
        "    M = norm_scale(M)\n",
        "    return M\n",
        "  \n",
        "  \n",
        "# Genrate dataset  \n",
        "def data_generator(T,C,le):\n",
        "    X_0 = []\n",
        "    X_1 = []\n",
        "    Y = []\n",
        "    for i in tqdm(range(len(T['pose']))): \n",
        "        p = np.copy(T['pose'][i])\n",
        "        p = zoom(p,target_l=C.frame_l,joints_num=C.joint_n,joints_dim=C.joint_d)\n",
        "\n",
        "        label = np.zeros(C.clc_num)\n",
        "        label[le.transform(T['label'])[i]-1] = 1   \n",
        "\n",
        "        M = get_CG(p,C)\n",
        "\n",
        "        X_0.append(M)\n",
        "        X_1.append(p)\n",
        "        Y.append(label)\n",
        "\n",
        "    X_0 = np.stack(X_0)  \n",
        "    X_1 = np.stack(X_1) \n",
        "    Y = np.stack(Y)\n",
        "    return X_0,X_1,Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDzVwCSYv6bS",
        "colab_type": "text"
      },
      "source": [
        "3. Define network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVHcN8sGav6m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def poses_diff(x):\n",
        "    H, W = x.get_shape()[1],x.get_shape()[2]\n",
        "    x = tf.subtract(x[:,1:,...],x[:,:-1,...])\n",
        "    x = tf.image.resize_nearest_neighbor(x,size=[H.value,W.value],align_corners=False) # should not alignment here\n",
        "    return x\n",
        "\n",
        "def pose_motion(P,frame_l):\n",
        "    P_diff_slow = Lambda(lambda x: poses_diff(x))(P)\n",
        "    P_diff_slow = Reshape((frame_l,-1))(P_diff_slow)\n",
        "    P_fast = Lambda(lambda x: x[:,::2,...])(P)\n",
        "    P_diff_fast = Lambda(lambda x: poses_diff(x))(P_fast)\n",
        "    P_diff_fast = Reshape((int(frame_l/2),-1))(P_diff_fast)\n",
        "    return P_diff_slow,P_diff_fast\n",
        "    \n",
        "def c1D(x,filters,kernel):\n",
        "    x = Conv1D(filters, kernel_size=kernel,padding='same',use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    return x\n",
        "\n",
        "def block(x,filters):\n",
        "    x = c1D(x,filters,3)\n",
        "    x = c1D(x,filters,3)\n",
        "    return x\n",
        "    \n",
        "def d1D(x,filters):\n",
        "    x = Dense(filters,use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    return x\n",
        "\n",
        "def build_FM(frame_l=32,joint_n=22,joint_d=2,feat_d=231,filters=16):   \n",
        "    M = Input(shape=(frame_l,feat_d))\n",
        "    P = Input(shape=(frame_l,joint_n,joint_d))\n",
        "    \n",
        "    diff_slow,diff_fast = pose_motion(P,frame_l)\n",
        "    \n",
        "    x = c1D(M,filters*2,1)\n",
        "    x = SpatialDropout1D(0.1)(x)\n",
        "    x = c1D(x,filters,3)\n",
        "    x = SpatialDropout1D(0.1)(x)\n",
        "    x = c1D(x,filters,1)\n",
        "    x = MaxPooling1D(2)(x)\n",
        "    x = SpatialDropout1D(0.1)(x)\n",
        "\n",
        "    x_d_slow = c1D(diff_slow,filters*2,1)\n",
        "    x_d_slow = SpatialDropout1D(0.1)(x_d_slow)\n",
        "    x_d_slow = c1D(x_d_slow,filters,3)\n",
        "    x_d_slow = SpatialDropout1D(0.1)(x_d_slow)\n",
        "    x_d_slow = c1D(x_d_slow,filters,1)\n",
        "    x_d_slow = MaxPool1D(2)(x_d_slow)\n",
        "    x_d_slow = SpatialDropout1D(0.1)(x_d_slow)\n",
        "        \n",
        "    x_d_fast = c1D(diff_fast,filters*2,1)\n",
        "    x_d_fast = SpatialDropout1D(0.1)(x_d_fast)\n",
        "    x_d_fast = c1D(x_d_fast,filters,3) \n",
        "    x_d_fast = SpatialDropout1D(0.1)(x_d_fast)\n",
        "    x_d_fast = c1D(x_d_fast,filters,1) \n",
        "    x_d_fast = SpatialDropout1D(0.1)(x_d_fast)\n",
        "   \n",
        "    x = concatenate([x,x_d_slow,x_d_fast])\n",
        "    x = block(x,filters*2)\n",
        "    x = MaxPool1D(2)(x)\n",
        "    x = SpatialDropout1D(0.1)(x)\n",
        "    \n",
        "    x = block(x,filters*4)\n",
        "    x = MaxPool1D(2)(x)\n",
        "    x = SpatialDropout1D(0.1)(x)\n",
        "\n",
        "    x = block(x,filters*8)\n",
        "    x = SpatialDropout1D(0.1)(x)\n",
        "    \n",
        "    return Model(inputs=[M,P],outputs=x)\n",
        "\n",
        "\n",
        "def build_DD_Net(C):\n",
        "    M = Input(name='M', shape=(C.frame_l,C.feat_d))  \n",
        "    P = Input(name='P', shape=(C.frame_l,C.joint_n,C.joint_d)) \n",
        "    \n",
        "    FM = build_FM(C.frame_l,C.joint_n,C.joint_d,C.feat_d,C.filters)\n",
        "    \n",
        "    x = FM([M,P])\n",
        "\n",
        "    x = GlobalMaxPool1D()(x)\n",
        "    \n",
        "    x = d1D(x,128)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = d1D(x,128)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(C.clc_num, activation='softmax')(x)\n",
        "    \n",
        "    ######################Self-supervised part\n",
        "    model = Model(inputs=[M,P],outputs=x)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8J0YBi3xf7r",
        "colab_type": "code",
        "outputId": "4becf768-1de0-4cf0-ba7c-7f2adc9c4327",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624
        }
      },
      "source": [
        "\n",
        "DD_Net = build_DD_Net(C)\n",
        "DD_Net.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "M (InputLayer)                  (None, 32, 105)      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "P (InputLayer)                  (None, 32, 15, 2)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "model_5 (Model)                 (None, 4, 512)       1714816     M[0][0]                          \n",
            "                                                                 P[0][0]                          \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_3 (GlobalM (None, 512)          0           model_5[1][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 128)          65536       global_max_pooling1d_3[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 128)          512         dense_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_50 (LeakyReLU)      (None, 128)          0           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 128)          0           leaky_re_lu_50[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 128)          16384       dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, 128)          512         dense_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_51 (LeakyReLU)      (None, 128)          0           batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 128)          0           leaky_re_lu_51[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 21)           2709        dropout_6[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 1,800,469\n",
            "Trainable params: 1,794,837\n",
            "Non-trainable params: 5,632\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "428x7yhRxyjR",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "4. Load dataset (download GT_train_1.pkl and  GT_test_1.pkl from github and then upload them )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fy2LRLhpICb",
        "colab_type": "code",
        "outputId": "3ddcb5a1-5be7-45f0-cf29-18236c42c30a",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "uploaded = google.colab.files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4f7ff1fb-98b3-402a-84ee-5d80af05f39e\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-4f7ff1fb-98b3-402a-84ee-5d80af05f39e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving GT_train_1.pkl to GT_train_1.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baR9WIVLye-0",
        "colab_type": "code",
        "outputId": "69bbe7c0-35b3-47c0-9e6f-8d22518a2a33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "Train = pickle.load(open(\"GT_train_1.pkl\", \"rb\"))\n",
        "Test = pickle.load(open(\"GT_test_1.pkl\", \"rb\"))\n",
        "\n",
        "from sklearn import preprocessing\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(Train['label'])\n",
        "\n",
        "X_0,X_1,Y = data_generator(Train,C,le)\n",
        "X_test_0,X_test_1,Y_test = data_generator(Test,C,le)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 433/433 [00:01<00:00, 243.81it/s]\n",
            "100%|██████████| 176/176 [00:00<00:00, 251.02it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJxAt-CQy5to",
        "colab_type": "text"
      },
      "source": [
        "5. Start train on split 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRwpfpB9zrB7",
        "colab_type": "code",
        "outputId": "43eca041-1900-40e5-f456-a258aca89ded",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import keras\n",
        "lr = 1e-3\n",
        "DD_Net.compile(loss=\"categorical_crossentropy\",optimizer=adam(lr),metrics=['accuracy'])\n",
        "lrScheduler = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, cooldown=5, min_lr=5e-6)\n",
        "history = DD_Net.fit([X_0,X_1],Y,\n",
        "                    batch_size=len(Y),\n",
        "                    epochs=600,\n",
        "                    verbose=True,\n",
        "                    shuffle=True,\n",
        "                    callbacks=[lrScheduler],\n",
        "                    validation_data=([X_test_0,X_test_1],Y_test)      \n",
        "                    )\n",
        "lr = 1e-4\n",
        "DD_Net.compile(loss=\"categorical_crossentropy\",optimizer=adam(lr),metrics=['accuracy'])\n",
        "lrScheduler = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, cooldown=5, min_lr=5e-6)\n",
        "history = DD_Net.fit([X_0,X_1],Y,\n",
        "                    batch_size=len(Y),\n",
        "                    epochs=600,\n",
        "                    verbose=True,\n",
        "                    shuffle=True,\n",
        "                    callbacks=[lrScheduler],\n",
        "                    validation_data=([X_test_0,X_test_1],Y_test)      \n",
        "                    )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 433 samples, validate on 176 samples\n",
            "Epoch 1/600\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "433/433 [==============================] - 15s 36ms/step - loss: 3.7831 - acc: 0.0462 - val_loss: 2.5920 - val_acc: 0.2443\n",
            "Epoch 2/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 3.2658 - acc: 0.1016 - val_loss: 2.3187 - val_acc: 0.3636\n",
            "Epoch 3/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 2.8780 - acc: 0.1570 - val_loss: 2.1587 - val_acc: 0.4148\n",
            "Epoch 4/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 2.8457 - acc: 0.1848 - val_loss: 2.1124 - val_acc: 0.3977\n",
            "Epoch 5/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 2.5933 - acc: 0.2471 - val_loss: 2.1052 - val_acc: 0.3864\n",
            "Epoch 6/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 2.4951 - acc: 0.2910 - val_loss: 2.0676 - val_acc: 0.4205\n",
            "Epoch 7/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 2.2783 - acc: 0.3210 - val_loss: 2.0165 - val_acc: 0.4432\n",
            "Epoch 8/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 2.2530 - acc: 0.3256 - val_loss: 1.9551 - val_acc: 0.4545\n",
            "Epoch 9/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 2.0642 - acc: 0.3926 - val_loss: 1.8759 - val_acc: 0.4602\n",
            "Epoch 10/600\n",
            "433/433 [==============================] - 2s 5ms/step - loss: 1.9746 - acc: 0.4480 - val_loss: 1.7717 - val_acc: 0.4659\n",
            "Epoch 11/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 1.9214 - acc: 0.4319 - val_loss: 1.6774 - val_acc: 0.4773\n",
            "Epoch 12/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 1.7685 - acc: 0.5058 - val_loss: 1.5968 - val_acc: 0.5341\n",
            "Epoch 13/600\n",
            "433/433 [==============================] - 2s 5ms/step - loss: 1.6687 - acc: 0.5173 - val_loss: 1.5286 - val_acc: 0.5455\n",
            "Epoch 14/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 1.6193 - acc: 0.5751 - val_loss: 1.4758 - val_acc: 0.5511\n",
            "Epoch 15/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 1.4765 - acc: 0.5774 - val_loss: 1.4332 - val_acc: 0.5795\n",
            "Epoch 16/600\n",
            "433/433 [==============================] - 2s 5ms/step - loss: 1.4267 - acc: 0.5935 - val_loss: 1.4129 - val_acc: 0.5909\n",
            "Epoch 17/600\n",
            "433/433 [==============================] - 2s 5ms/step - loss: 1.3712 - acc: 0.6189 - val_loss: 1.3902 - val_acc: 0.5966\n",
            "Epoch 18/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 1.3750 - acc: 0.6074 - val_loss: 1.3587 - val_acc: 0.6080\n",
            "Epoch 19/600\n",
            "433/433 [==============================] - 2s 5ms/step - loss: 1.2176 - acc: 0.6559 - val_loss: 1.3249 - val_acc: 0.6193\n",
            "Epoch 20/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 1.2020 - acc: 0.6790 - val_loss: 1.2968 - val_acc: 0.6420\n",
            "Epoch 21/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 1.1302 - acc: 0.7136 - val_loss: 1.2689 - val_acc: 0.6648\n",
            "Epoch 22/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 1.0439 - acc: 0.7252 - val_loss: 1.2540 - val_acc: 0.6875\n",
            "Epoch 23/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.9965 - acc: 0.7691 - val_loss: 1.2467 - val_acc: 0.6875\n",
            "Epoch 24/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.9413 - acc: 0.7483 - val_loss: 1.2423 - val_acc: 0.6761\n",
            "Epoch 25/600\n",
            "433/433 [==============================] - 3s 6ms/step - loss: 0.9243 - acc: 0.7714 - val_loss: 1.2204 - val_acc: 0.6875\n",
            "Epoch 26/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.9130 - acc: 0.7691 - val_loss: 1.1856 - val_acc: 0.6932\n",
            "Epoch 27/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.8290 - acc: 0.7806 - val_loss: 1.1679 - val_acc: 0.6818\n",
            "Epoch 28/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.7769 - acc: 0.8291 - val_loss: 1.1628 - val_acc: 0.6818\n",
            "Epoch 29/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.7318 - acc: 0.8245 - val_loss: 1.1580 - val_acc: 0.6761\n",
            "Epoch 30/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.7336 - acc: 0.8637 - val_loss: 1.1609 - val_acc: 0.6875\n",
            "Epoch 31/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.6982 - acc: 0.8176 - val_loss: 1.1602 - val_acc: 0.6761\n",
            "Epoch 32/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.6770 - acc: 0.8499 - val_loss: 1.1580 - val_acc: 0.6761\n",
            "Epoch 33/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.6399 - acc: 0.8499 - val_loss: 1.1547 - val_acc: 0.6705\n",
            "Epoch 34/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.5737 - acc: 0.8707 - val_loss: 1.1567 - val_acc: 0.6648\n",
            "Epoch 35/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.5800 - acc: 0.8799 - val_loss: 1.1600 - val_acc: 0.6761\n",
            "Epoch 36/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.5616 - acc: 0.8891 - val_loss: 1.1628 - val_acc: 0.6818\n",
            "Epoch 37/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.5533 - acc: 0.8637 - val_loss: 1.1685 - val_acc: 0.6875\n",
            "Epoch 38/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.5330 - acc: 0.8845 - val_loss: 1.1776 - val_acc: 0.6875\n",
            "Epoch 39/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.4819 - acc: 0.9030 - val_loss: 1.1761 - val_acc: 0.6875\n",
            "Epoch 40/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.4721 - acc: 0.8961 - val_loss: 1.1749 - val_acc: 0.6932\n",
            "Epoch 41/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.4279 - acc: 0.9238 - val_loss: 1.1693 - val_acc: 0.6989\n",
            "Epoch 42/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.4011 - acc: 0.9284 - val_loss: 1.1612 - val_acc: 0.7045\n",
            "Epoch 43/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.4001 - acc: 0.9192 - val_loss: 1.1620 - val_acc: 0.7102\n",
            "Epoch 44/600\n",
            "433/433 [==============================] - 2s 5ms/step - loss: 0.3726 - acc: 0.9284 - val_loss: 1.1683 - val_acc: 0.7102\n",
            "Epoch 45/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.3645 - acc: 0.9330 - val_loss: 1.1874 - val_acc: 0.7159\n",
            "Epoch 46/600\n",
            "433/433 [==============================] - 2s 5ms/step - loss: 0.3150 - acc: 0.9515 - val_loss: 1.2057 - val_acc: 0.7102\n",
            "Epoch 47/600\n",
            "433/433 [==============================] - 2s 5ms/step - loss: 0.3643 - acc: 0.9330 - val_loss: 1.2250 - val_acc: 0.7102\n",
            "Epoch 48/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.3328 - acc: 0.9400 - val_loss: 1.2571 - val_acc: 0.6932\n",
            "Epoch 49/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.2996 - acc: 0.9423 - val_loss: 1.2627 - val_acc: 0.7045\n",
            "Epoch 50/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.3188 - acc: 0.9400 - val_loss: 1.2581 - val_acc: 0.7045\n",
            "Epoch 51/600\n",
            "433/433 [==============================] - 2s 5ms/step - loss: 0.2581 - acc: 0.9677 - val_loss: 1.2385 - val_acc: 0.6989\n",
            "Epoch 52/600\n",
            "433/433 [==============================] - 2s 5ms/step - loss: 0.2979 - acc: 0.9423 - val_loss: 1.2314 - val_acc: 0.7045\n",
            "Epoch 53/600\n",
            "433/433 [==============================] - 2s 5ms/step - loss: 0.2633 - acc: 0.9561 - val_loss: 1.2237 - val_acc: 0.7045\n",
            "Epoch 54/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.2568 - acc: 0.9469 - val_loss: 1.2082 - val_acc: 0.7102\n",
            "Epoch 55/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.2252 - acc: 0.9607 - val_loss: 1.1886 - val_acc: 0.7216\n",
            "Epoch 56/600\n",
            "433/433 [==============================] - 2s 5ms/step - loss: 0.2397 - acc: 0.9700 - val_loss: 1.1613 - val_acc: 0.7216\n",
            "Epoch 57/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.2124 - acc: 0.9700 - val_loss: 1.1336 - val_acc: 0.7273\n",
            "Epoch 58/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.2189 - acc: 0.9700 - val_loss: 1.1283 - val_acc: 0.7443\n",
            "Epoch 59/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.2000 - acc: 0.9746 - val_loss: 1.1299 - val_acc: 0.7386\n",
            "Epoch 60/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.1968 - acc: 0.9723 - val_loss: 1.1305 - val_acc: 0.7273\n",
            "Epoch 61/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.1899 - acc: 0.9677 - val_loss: 1.1313 - val_acc: 0.7273\n",
            "Epoch 62/600\n",
            "433/433 [==============================] - 3s 6ms/step - loss: 0.1866 - acc: 0.9630 - val_loss: 1.1248 - val_acc: 0.7330\n",
            "Epoch 63/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.1730 - acc: 0.9769 - val_loss: 1.1216 - val_acc: 0.7330\n",
            "Epoch 64/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.1871 - acc: 0.9769 - val_loss: 1.1218 - val_acc: 0.7330\n",
            "Epoch 65/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.1683 - acc: 0.9630 - val_loss: 1.1347 - val_acc: 0.7273\n",
            "Epoch 66/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.1772 - acc: 0.9700 - val_loss: 1.1479 - val_acc: 0.7102\n",
            "Epoch 67/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.1403 - acc: 0.9861 - val_loss: 1.1637 - val_acc: 0.7045\n",
            "Epoch 68/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.1493 - acc: 0.9792 - val_loss: 1.1739 - val_acc: 0.6989\n",
            "Epoch 69/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.1598 - acc: 0.9815 - val_loss: 1.1931 - val_acc: 0.7045\n",
            "Epoch 70/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.1506 - acc: 0.9792 - val_loss: 1.2035 - val_acc: 0.7102\n",
            "Epoch 71/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.1354 - acc: 0.9861 - val_loss: 1.1880 - val_acc: 0.7273\n",
            "Epoch 72/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.1351 - acc: 0.9861 - val_loss: 1.1716 - val_acc: 0.7557\n",
            "Epoch 73/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.1287 - acc: 0.9861 - val_loss: 1.1751 - val_acc: 0.7614\n",
            "Epoch 74/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.1205 - acc: 0.9815 - val_loss: 1.1868 - val_acc: 0.7557\n",
            "Epoch 75/600\n",
            "433/433 [==============================] - 2s 5ms/step - loss: 0.1086 - acc: 0.9861 - val_loss: 1.1976 - val_acc: 0.7500\n",
            "Epoch 76/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.1028 - acc: 0.9931 - val_loss: 1.2105 - val_acc: 0.7443\n",
            "Epoch 77/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.1161 - acc: 0.9861 - val_loss: 1.2182 - val_acc: 0.7273\n",
            "Epoch 78/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.1098 - acc: 0.9908 - val_loss: 1.2271 - val_acc: 0.7330\n",
            "Epoch 79/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0938 - acc: 0.9977 - val_loss: 1.2356 - val_acc: 0.7330\n",
            "Epoch 80/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.1128 - acc: 0.9908 - val_loss: 1.2367 - val_acc: 0.7386\n",
            "Epoch 81/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.1297 - acc: 0.9815 - val_loss: 1.2537 - val_acc: 0.7500\n",
            "Epoch 82/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0984 - acc: 0.9977 - val_loss: 1.2826 - val_acc: 0.7386\n",
            "Epoch 83/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.1015 - acc: 0.9885 - val_loss: 1.2884 - val_acc: 0.7273\n",
            "Epoch 84/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.1030 - acc: 0.9931 - val_loss: 1.2830 - val_acc: 0.7386\n",
            "Epoch 85/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0907 - acc: 0.9908 - val_loss: 1.2726 - val_acc: 0.7386\n",
            "Epoch 86/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0829 - acc: 0.9931 - val_loss: 1.2571 - val_acc: 0.7386\n",
            "Epoch 87/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0835 - acc: 0.9977 - val_loss: 1.2408 - val_acc: 0.7386\n",
            "Epoch 88/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0834 - acc: 0.9908 - val_loss: 1.2197 - val_acc: 0.7500\n",
            "Epoch 89/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0836 - acc: 0.9931 - val_loss: 1.1900 - val_acc: 0.7614\n",
            "Epoch 90/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0866 - acc: 0.9954 - val_loss: 1.1704 - val_acc: 0.7614\n",
            "Epoch 91/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0885 - acc: 0.9954 - val_loss: 1.1544 - val_acc: 0.7614\n",
            "Epoch 92/600\n",
            "433/433 [==============================] - 3s 6ms/step - loss: 0.0938 - acc: 0.9908 - val_loss: 1.1331 - val_acc: 0.7784\n",
            "Epoch 93/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0875 - acc: 0.9954 - val_loss: 1.1096 - val_acc: 0.7898\n",
            "Epoch 94/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0873 - acc: 0.9954 - val_loss: 1.0934 - val_acc: 0.7955\n",
            "Epoch 95/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0828 - acc: 0.9931 - val_loss: 1.0813 - val_acc: 0.7955\n",
            "Epoch 96/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0790 - acc: 0.9954 - val_loss: 1.0690 - val_acc: 0.7955\n",
            "Epoch 97/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0890 - acc: 0.9931 - val_loss: 1.0568 - val_acc: 0.7955\n",
            "Epoch 98/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0956 - acc: 0.9908 - val_loss: 1.0436 - val_acc: 0.7955\n",
            "Epoch 99/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0813 - acc: 0.9977 - val_loss: 1.0310 - val_acc: 0.8011\n",
            "Epoch 100/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0761 - acc: 0.9954 - val_loss: 1.0187 - val_acc: 0.8011\n",
            "Epoch 101/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0781 - acc: 0.9954 - val_loss: 1.0076 - val_acc: 0.7955\n",
            "Epoch 102/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0698 - acc: 0.9931 - val_loss: 0.9968 - val_acc: 0.8011\n",
            "Epoch 103/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0644 - acc: 1.0000 - val_loss: 0.9873 - val_acc: 0.8011\n",
            "Epoch 104/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0704 - acc: 0.9977 - val_loss: 0.9813 - val_acc: 0.8011\n",
            "Epoch 105/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0803 - acc: 0.9954 - val_loss: 0.9764 - val_acc: 0.8011\n",
            "Epoch 106/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0762 - acc: 0.9977 - val_loss: 0.9705 - val_acc: 0.8011\n",
            "Epoch 107/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0800 - acc: 0.9954 - val_loss: 0.9651 - val_acc: 0.8011\n",
            "Epoch 108/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0700 - acc: 0.9931 - val_loss: 0.9600 - val_acc: 0.8068\n",
            "Epoch 109/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0756 - acc: 0.9977 - val_loss: 0.9563 - val_acc: 0.8068\n",
            "Epoch 110/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0643 - acc: 0.9977 - val_loss: 0.9544 - val_acc: 0.8068\n",
            "Epoch 111/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0794 - acc: 0.9931 - val_loss: 0.9522 - val_acc: 0.8068\n",
            "Epoch 112/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0654 - acc: 0.9977 - val_loss: 0.9506 - val_acc: 0.8068\n",
            "Epoch 113/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0735 - acc: 0.9931 - val_loss: 0.9494 - val_acc: 0.8068\n",
            "Epoch 114/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0609 - acc: 0.9977 - val_loss: 0.9480 - val_acc: 0.8068\n",
            "Epoch 115/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0720 - acc: 0.9977 - val_loss: 0.9471 - val_acc: 0.8068\n",
            "Epoch 116/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0762 - acc: 0.9977 - val_loss: 0.9459 - val_acc: 0.8068\n",
            "Epoch 117/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0793 - acc: 0.9954 - val_loss: 0.9448 - val_acc: 0.8068\n",
            "Epoch 118/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0716 - acc: 0.9954 - val_loss: 0.9442 - val_acc: 0.8011\n",
            "Epoch 119/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0785 - acc: 0.9954 - val_loss: 0.9438 - val_acc: 0.8011\n",
            "Epoch 120/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0884 - acc: 0.9977 - val_loss: 0.9428 - val_acc: 0.8011\n",
            "Epoch 121/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0670 - acc: 1.0000 - val_loss: 0.9415 - val_acc: 0.8011\n",
            "Epoch 122/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0669 - acc: 0.9931 - val_loss: 0.9403 - val_acc: 0.8011\n",
            "Epoch 123/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0745 - acc: 0.9977 - val_loss: 0.9387 - val_acc: 0.8011\n",
            "Epoch 124/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0655 - acc: 0.9977 - val_loss: 0.9371 - val_acc: 0.8011\n",
            "Epoch 125/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0710 - acc: 0.9908 - val_loss: 0.9356 - val_acc: 0.8068\n",
            "Epoch 126/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0614 - acc: 0.9977 - val_loss: 0.9346 - val_acc: 0.8068\n",
            "Epoch 127/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0809 - acc: 0.9954 - val_loss: 0.9333 - val_acc: 0.8068\n",
            "Epoch 128/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0695 - acc: 0.9977 - val_loss: 0.9314 - val_acc: 0.8068\n",
            "Epoch 129/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0856 - acc: 0.9885 - val_loss: 0.9293 - val_acc: 0.8068\n",
            "Epoch 130/600\n",
            "433/433 [==============================] - 3s 6ms/step - loss: 0.0788 - acc: 0.9954 - val_loss: 0.9272 - val_acc: 0.8068\n",
            "Epoch 131/600\n",
            "433/433 [==============================] - 3s 6ms/step - loss: 0.0772 - acc: 1.0000 - val_loss: 0.9249 - val_acc: 0.8068\n",
            "Epoch 132/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0664 - acc: 0.9977 - val_loss: 0.9228 - val_acc: 0.8068\n",
            "Epoch 133/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0605 - acc: 1.0000 - val_loss: 0.9206 - val_acc: 0.8068\n",
            "Epoch 134/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0728 - acc: 0.9954 - val_loss: 0.9183 - val_acc: 0.8068\n",
            "Epoch 135/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0717 - acc: 1.0000 - val_loss: 0.9162 - val_acc: 0.8068\n",
            "Epoch 136/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0552 - acc: 0.9977 - val_loss: 0.9140 - val_acc: 0.8068\n",
            "Epoch 137/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0724 - acc: 0.9931 - val_loss: 0.9117 - val_acc: 0.8068\n",
            "Epoch 138/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0677 - acc: 0.9954 - val_loss: 0.9095 - val_acc: 0.8068\n",
            "Epoch 139/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0853 - acc: 0.9885 - val_loss: 0.9075 - val_acc: 0.8068\n",
            "Epoch 140/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0711 - acc: 0.9931 - val_loss: 0.9052 - val_acc: 0.8068\n",
            "Epoch 141/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0643 - acc: 0.9977 - val_loss: 0.9028 - val_acc: 0.8068\n",
            "Epoch 142/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0586 - acc: 0.9977 - val_loss: 0.9008 - val_acc: 0.8068\n",
            "Epoch 143/600\n",
            "433/433 [==============================] - 2s 5ms/step - loss: 0.0710 - acc: 0.9954 - val_loss: 0.8986 - val_acc: 0.8068\n",
            "Epoch 144/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0699 - acc: 0.9977 - val_loss: 0.8965 - val_acc: 0.8068\n",
            "Epoch 145/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0625 - acc: 0.9908 - val_loss: 0.8945 - val_acc: 0.8068\n",
            "Epoch 146/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0727 - acc: 0.9977 - val_loss: 0.8927 - val_acc: 0.8068\n",
            "Epoch 147/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0698 - acc: 0.9954 - val_loss: 0.8910 - val_acc: 0.8068\n",
            "Epoch 148/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0614 - acc: 0.9977 - val_loss: 0.8896 - val_acc: 0.8125\n",
            "Epoch 149/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0776 - acc: 0.9931 - val_loss: 0.8879 - val_acc: 0.8125\n",
            "Epoch 150/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0754 - acc: 0.9977 - val_loss: 0.8862 - val_acc: 0.8125\n",
            "Epoch 151/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0578 - acc: 1.0000 - val_loss: 0.8845 - val_acc: 0.8125\n",
            "Epoch 152/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0645 - acc: 0.9954 - val_loss: 0.8829 - val_acc: 0.8125\n",
            "Epoch 153/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0654 - acc: 0.9977 - val_loss: 0.8814 - val_acc: 0.8125\n",
            "Epoch 154/600\n",
            "433/433 [==============================] - 2s 5ms/step - loss: 0.0765 - acc: 0.9885 - val_loss: 0.8799 - val_acc: 0.8125\n",
            "Epoch 155/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0686 - acc: 0.9954 - val_loss: 0.8782 - val_acc: 0.8125\n",
            "Epoch 156/600\n",
            "433/433 [==============================] - 2s 6ms/step - loss: 0.0656 - acc: 1.0000 - val_loss: 0.8764 - val_acc: 0.8125\n",
            "Epoch 157/600\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Tx0Txay0v5-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8Rrxx5M0x4c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}